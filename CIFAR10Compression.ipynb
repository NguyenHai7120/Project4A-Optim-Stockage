{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Dataset Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Outils.dataloader import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Outils.dataloader import load_CIFAR10\n",
    "\n",
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'Dataset/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "X = np.concatenate([X_train, X_test])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll load the clusters obtained from KMeans Clustering algorithm for different number of clusters into a dictionary for comparison purpose.\n",
    "\n",
    "Note that the clusters are saved in `.\\Clusters\\` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KMeans10': array([ 8, 28, 14, ..., 45, 33, 79]),\n",
       " 'KMeans20': array([17,  3,  0, ...,  6, 13,  1]),\n",
       " 'KMeans30': array([28, 18, 27, ..., 13, 20, 11]),\n",
       " 'KMeans40': array([13, 34, 11, ...,  9, 37, 19]),\n",
       " 'KMeans50': array([26, 19,  3, ..., 11, 30, 45]),\n",
       " 'KMeans60': array([16, 47, 55, ...,  6, 11, 21]),\n",
       " 'KMeans70': array([46, 39, 32, ..., 28,  3, 47]),\n",
       " 'KMeans80': array([ 8, 60, 59, ..., 48, 10, 61]),\n",
       " 'KMeans90': array([74, 85,  6, ..., 50, 44, 34])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_path = \"./Clusters/*.npy\"   # Path to all .npy files\n",
    "clusters = {}\n",
    "for f in glob.glob(cluster_path):\n",
    "    clusters[f[11:19]] = np.load(f)\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Compression Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Outils.compression import single_compression\n",
    "import min_max_diff_flatten as mmdf \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the image sets (clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare the directories for saving the compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "\n",
    "        path = \".\\Dataset\\Compressed\\mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            # os.rmdir(path)\n",
    "            files = glob.glob(path + '*')\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "        else:\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "        image_set = X[cluster == value]\n",
    "        compressed_set, min, max = mmdf.Encoder(image_set)\n",
    "\n",
    "        # os.chdir(path)\n",
    "        path = \".\\Dataset\\Compressed\\mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        for i in range(compressed_set.shape[0]):\n",
    "            cv2.imwrite(path + \"cluster\" + str(value) + 'im' + str(i) + \".png\", compressed_set[i].reshape((32,32,-1)))\n",
    "        \n",
    "        cv2.imwrite(path + \"min\" + str(value) + \".png\", min.reshape((32,32,-1)))\n",
    "        cv2.imwrite(path + \"max\" + str(value) + \".png\", max.reshape((32,32,-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get size of compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name in clusters:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151919381\n",
      "152633064\n",
      "153323341\n",
      "154012987\n",
      "154616893\n",
      "155083185\n"
     ]
    }
   ],
   "source": [
    "for name in clusters:\n",
    "    path = \".\\Dataset\\Compressed\\mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "    size = 0\n",
    "    for f in glob.glob(path + \"*.png\"):\n",
    "        size = size + os.path.getsize(f)\n",
    "    \n",
    "    print(size)\n",
    "    np.save(\"./Saved_datas/mmdf_\" + name + \"_png_size.npy\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import min_max_predictive as mmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare the directories for saving the compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans10\n",
      "KMeans20\n",
      "KMeans30\n",
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "\n",
    "        path = \".\\Dataset\\Compressed\\mmp_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            # os.rmdir(path)\n",
    "            files = glob.glob(path + '*')\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "        else:\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans10\n",
      "KMeans20\n",
      "KMeans30\n",
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "        image_set = X[cluster == value]\n",
    "        compressed_set, level, min, max = mmp.Encoder(image_set)\n",
    "\n",
    "        # os.chdir(path)\n",
    "        path = \".\\Dataset\\Compressed\\mmp_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        for i in range(compressed_set.shape[0]):\n",
    "            cv2.imwrite(path + \"cluster\" + str(value) + 'im' + str(i) + \".png\", compressed_set[i])\n",
    "        \n",
    "        np.save(path + \"level\" + str(value) + \".npy\", level)\n",
    "        cv2.imwrite(path + \"min\" + str(value) + \".png\", min)\n",
    "        cv2.imwrite(path + \"max\" + str(value) + \".png\", max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get size of compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334790159\n",
      "332244375\n",
      "332712277\n",
      "333148966\n",
      "333435277\n",
      "333821840\n",
      "334086773\n",
      "334339200\n",
      "334568023\n"
     ]
    }
   ],
   "source": [
    "for name in clusters:\n",
    "    path = \".\\Dataset\\Compressed\\mmp_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "    size = 0\n",
    "    for f in glob.glob(path + \"*.png\"):\n",
    "        size = size + os.path.getsize(f)\n",
    "    for f in glob.glob(path + \"*.npy\"):\n",
    "        size = size + os.path.getsize(f)\n",
    "    \n",
    "    print(size)\n",
    "    np.save(\"./Saved_datas/mmdf_\" + name + \"_png_size.npy\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with Delta Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare the directories for saving the compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans10\n",
      "KMeans20\n",
      "KMeans30\n",
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "\n",
    "        path = \".\\Dataset\\Compressed\\delta_mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            # os.rmdir(path)\n",
    "            files = glob.glob(path + '*')\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "        else:\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans10\n",
      "KMeans20\n",
      "KMeans30\n",
      "KMeans40\n",
      "KMeans50\n",
      "KMeans60\n",
      "KMeans70\n",
      "KMeans80\n",
      "KMeans90\n"
     ]
    }
   ],
   "source": [
    "for name, cluster in clusters.items():\n",
    "    print(name)\n",
    "    for value in np.unique(cluster):\n",
    "        image_set = X[cluster == value]\n",
    "        image_set = delta.Delta_Encoder(image_set)\n",
    "        compressed_set, min, max = mmdf.Encoder(image_set)\n",
    "        \n",
    "\n",
    "        # os.chdir(path)\n",
    "        path = \".\\Dataset\\Compressed\\delta_mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "        for i in range(compressed_set.shape[0]):\n",
    "            cv2.imwrite(path + \"cluster\" + str(value) + 'im' + str(i) + \".png\", compressed_set[i])\n",
    "        \n",
    "        cv2.imwrite(path + \"min\" + str(value) + \".png\", min)\n",
    "        cv2.imwrite(path + \"max\" + str(value) + \".png\", max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get size of compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166271795\n",
      "162732042\n",
      "163828303\n",
      "164291782\n",
      "164934439\n",
      "165173088\n",
      "165587916\n",
      "165915535\n",
      "166092224\n"
     ]
    }
   ],
   "source": [
    "for name in clusters:\n",
    "    path = \".\\Dataset\\Compressed\\delta_mmdf_png\\ \" + name  + \"\\ \"\n",
    "\n",
    "    size = 0\n",
    "    for f in glob.glob(path + \"*.png\"):\n",
    "        size = size + os.path.getsize(f)\n",
    "    \n",
    "    print(size)\n",
    "    np.save(\"./Saved_datas/delta_mmdf\" + name + \"_png_size.npy\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta and PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare the directories for saving the compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_png = \".\\Dataset\\Compressed\\delta_png\\ \"\n",
    "\n",
    "if os.path.exists(path_png):\n",
    "    # os.rmdir(path_png)\n",
    "    files = glob.glob(path_png + '*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "else:\n",
    "    os.mkdir(path_png)\n",
    "\n",
    "# os.chdir(path_png)\n",
    "\n",
    "X_delta = delta.Delta_Encoder(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    cv2.imwrite(path_png + str(i) + \".png\", X_delta[i,:,:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get size of compressed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95154655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \".\\Dataset\\Compressed\\delta_png\\ \"\n",
    "\n",
    "size = 0\n",
    "for f in glob.glob(path + \"*.png\"):\n",
    "    size = size + os.path.getsize(f)\n",
    "\n",
    "print(size)\n",
    "np.save(\"./Saved_datas/delta_png_size.npy\", size)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ccd7312afbc7f6b085d2146f709040387adf44f7120b2b3d2b8d45900fc5481"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
